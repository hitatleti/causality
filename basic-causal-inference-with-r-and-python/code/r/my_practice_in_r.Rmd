---
title: "my_practice_in_R"
author: "Hisanori Tanaka"
date: "2023-04-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Chapter01

#### 1.9　simple exercise in R

```{r 1.9.1, include=TRUE}
rm(list = ls()) # ワークスペースをクリアにする
data01 = read.csv("../../data/data01.csv")
data01
# head(data01)とすれば, 最初の6行ぶんを表示
```

summary関数を使って基本統計量を表示

```{r 1.9.2, include=TRUE}
summary(data01)
```

## Chapter02　潜在的結果変数の枠組み


#### 2.1　潜在的結果変数の枠組み：具体例

単純な「前後比較」や「集団ごとの平均の差の比較」によって因果を推し測ろうとすることはよくある誤り. <br>
統計的因果推論では, 潜在的結果の差から計算される因果効果を推定する. （ただし, 実際に潜在的結果は観測されないことに注意. ）

#### 2.2　潜在的結果変数の枠組み：理論

```{r 2.2.1, include=TRUE}
rm(list = ls())
data02 = read.csv("../../data/data02.csv")
attach(data02)
head(data02)
```

```{r 2.2.2, include=TRUE}
summary(data02)
```

#### 2.3　処置効果１：個体因果効果

<div style="display: inline-block; background: gray; padding: 3px 10px; color: #ffffff;">
<strong> Def 2.3：個体因果効果（ICE: individual causal effect） </strong> </div>
<div style="padding: 10px; border: 2px solid gray;"> 
$$ \tau_{i} = \gamma_{i}(1) - \gamma_{i}(0) $$ </div> <br>


```{r 2.3.1, include=TRUE}
y1t - y0t
```

```{r 2.3.2, include=TRUE}
y3 - x1
```

```{r 2.3.3, include=TRUE}
y1 - y0
```

潜在的結果は一方が観測された時, 他方は観測されないため, ITEは定義できても観測も推定もできない. 

#### 2.4　処置効果２：平均処置効果

<div style="display: inline-block; background: gray; padding: 3px 10px; color: #ffffff;">
<strong> Def 2.4：個体因果効果（ICE: individual causal effect）あるいは 平均処置効果（ATE） </strong> </div>
<div style="padding: 10px; border: 2px solid gray;"> 
$$ \tau_{\mathrm{ATE}} = \mathrm{E} [ \gamma_{i}(1) - \gamma_{i}(0) ] = \mathrm{E} [ \gamma_{i}(1)] - \mathrm{E} [ \gamma_{i}(0)] $$ </div> <br>

```{r 2.4.1, include=TRUE}
mean(y1t) - mean(y0t)
```
```{r 2.4.2, include=TRUE}
mean(y3) - mean(x1)
```
```{r 2.4.3, include=TRUE}
m1 = mean(y1, na.rm = TRUE)
m0 = mean(y0, na.rm = TRUE)
m1 - m0
```

#### 2.5　処置効果３：処置群の平均処置効果

<div style="display: inline-block; background: gray; padding: 3px 10px; color: #ffffff;">
<strong> Def 2.5：処置群の平均処置効果（ATT: average treatment effect on the treated） </strong> </div>
<div style="padding: 10px; border: 2px solid gray;"> 
$$ \tau_{\mathrm{ATT}} = \mathrm{E} [ \gamma_{i}(1) - \gamma_{i}(0) | T_{i} = 1] = \mathrm{E} [ \gamma_{i}(1) | T_{i} = 1] - \mathrm{E} [ \gamma_{i}(0) | T_{i} = 1] $$ </div> <br>


```{r 2.5.1, include=TRUE}
# 真のATT
mt1 = mean(y1t[t1 = 1])
mt0 = mean(y0t[t1 = 1])
mt1 - mt0
```

処置が無作為である　$\Rightarrow$　ATE $=$ ATT <br>
処置が無作為でない　$\Rightarrow$　ATE $\neq$ ATT　となる可能性がある <br>

#### 2.6　交絡因子

単純な推定量にはほとんどの場合何らかの変数による交絡（confounding）がある. 統計的因果推論では, 処置の割付けがどのように行われているのかは非常に重要である. 

##### 2.6.2　方向付き非巡回グラフ（DAG）

因果関係や交絡の有無を視覚化する方法として, 方向付き非巡回グラフ（DAG: Directed Acyclic Graph）というものがある. 

#### 2.7　無作為抽出と無作為割付け

<div style="display: inline-block; background: gray; padding: 3px 10px; color: #ffffff;">
<strong> Def 2.7：無作為割付け </strong> </div>
<div style="padding: 10px; border: 2px solid gray;"> 
無作為割付けとは, 標本データを処置群と統制群に無作為に分けることである </div> <br>

無作為抽出の割付Ver.と考えればよい. <br>

また, 処置群と統制群に無作為割付けする研究方法を実験研究, そうでない研究方法を観察研究と呼ぶ. 

#### 2.8　無作為割付けによる分析の例

```{r 2.8.1, include=TRUE}
set.seed(1)
r0 = runif(20, 0, 1)
r1 = round(r0, 0)
y2 = NULL
y2[r1 == 1] = y1t[r1 == 1]
y2[r1 == 0] = y0t[r1 == 0]
```

```{r 2.8.2, include=TRUE}
r1
```

```{r 2.8.3, include=TRUE}
y2
```

```{r 2.8.4, include=TRUE}
mr1 = mean(y2[r1 == 1])
mr0 = mean(y2[r1 == 0])
mr1 - mr0
```

#### 2.10　２標本$t$検定

```{r 2.10.1, include=TRUE}
t.test(y2[r1==1], y2[r1==0], var.equal = FALSE)
```

### Chapter03　統計的因果推論における重要な仮定

#### 3.1　STUVA

<div style="display: inline-block; background: gray; padding: 3px 10px; color: #ffffff;">
<strong> Def 3.1：STUVA（stable unit treatment value assumption） </strong> </div>
<div style="padding: 10px; border: 2px solid gray;"> 
SUTVAとは, 以下の2つの条件を満たすことである. <br>

<ol>
<li>相互干渉がない
<li>個体に対する隠れた処置がない
</div> <br>

#### 3.8　共変量の役割

<div style="display: inline-block; background: gray; padding: 3px 10px; color: #ffffff;">
<strong> Def 3.8：無交絡性（unconfoundedness） </strong> </div>
<div style="padding: 10px; border: 2px solid gray;"> 
処置の割付けを表す変数$T_{i}$が, 潜在的結果変数の組$\{ \gamma_{i}(1), \gamma_{i}(0) \}$に依存しないことを条件付き独立または無交絡性という. 
$$ \{ \gamma_{i}(1), \gamma_{i}(0) \} \perp T_{i} | \bf{X} $$ </div> <br>

```{r 3.8.1, include=TRUE}
rm(list = ls())
data03 = read.csv("../../data/data03.csv")
attach(data03)
head(data03)
```

```{r 3.8.2, include=TRUE}
summary(data03)
```

```{r 3.8.3, include=TRUE}
mean(y3[t1==1]) - mean(y3[t1==0])
mean(y1t) - mean(y0t)
```

「無視可能な割付け」とは, ナイーブな推定量によって文字通りに「割付けを無視して解析してよい」ということを意味するわけではない. 

### Chapter05　回帰分析の基礎

#### 5.2　数値例で理解する最小二乗法

```{r 5.2.1, include=TRUE}
rm(list = ls())
y1 = c(40, 20, 50, 10)
x1 = c(5, 1, 3, 2)
```

```{r 5.2.2, include=TRUE}
yhat1 = 11.143 + 6.857 * x1
e1 = y1 - yhat1
print(yhat1)
print(e1)
```
```{r 5.2.3, include=TRUE}
yhat2 = 10.909 * x1
e2 = y1 - yhat2
print(yhat2)
print(e2)
```
```{r 5.2.4, include=TRUE}
# 残差の合計は常にゼロである（以下の例では, 丸め誤差のためにゼロにはなっていない）
sum(e1)
sum(e2)
```

```{r 5.2.5, include=TRUE}
sum(e1^2)
sum(e2^2)
```

```{r 5.2.6, include=TRUE}
model1 = lm(y1 ~ x1)
summary(model1)
```

#### 5.4　条件付き期待値としての回帰モデル

```{r 5.4.1, include=TRUE}
rm(list = ls())
y1 = c(seq(9))
x1 = c(rep(1, 3), rep(2, 3), rep(3, 3))
mean(y1)
```
```{r 5.4.2, include=TRUE}
model2 = lm(y1 ~ x1)
summary(model2)
```
```{r 5.4.3, include=TRUE}
plot(x1, y1)
abline(model2)
```

### Chapter06　図で理解する重回帰モデルの基礎

```{r 6.1.1, include=TRUE}
rm(list = ls())
data06 = read.csv("../../data/data06.csv")
attach(data06)
head(data06)
```

```{r 6.1.2, include=TRUE}
summary(data06)
```

```{r 6.3.1, include=TRUE}
summary(data06)
```

### Chapter07　最小二乗法による重回帰モデルの仮定と診断１

```{r 7.2.1, include=TRUE}
rm(list = ls())
data07a = read.csv("../../data/data07a.csv")
head(data07a)
```

#### 7.3　仮定３：誤差項の条件付き期待値ゼロ

##### 7.3.1　不要なモデルを取り入れる問題

```{r 7.3.1.1, include=TRUE}
rm(list = ls())
data07c = read.csv("../../data/data07c.csv")
head(data07c)
```

```{r 7.3.1.2, include=TRUE}
summary(data07c)
```

```{r 7.3.1.3, include=TRUE}
model17 = lm(y1 ~ x1, data=data07c)
model18 = lm(y1 ~ x1 + x2, data=data07c)
model19 = lm(y1 ~ x1 + x2 + x3, data=data07c)
summary(model17)
summary(model18)
summary(model19)
```

```{r 7.3.1.4, include=TRUE}
confint(model18)
confint(model19)
```

##### 7.3.2　中間変数をモデルに取り入れる問題

```{r 7.3.2.1, include=TRUE}
rm(list = ls())
data07d = read.csv("../../data/data07d.csv")
head(data07d)
summary(data07d)
```

$$ X_{2} = 1.0 + 1.5 X_{1} + e_{1} \\ 
   Y_{1} = 1.0 + 1.3X_{1} + 1.2X_{2} + e_{2} $$


```{r 7.3.2.2, include=TRUE}
model20 = lm(y1 ~ x1, data=data07d)
model21 = lm(y1 ~ x1 + x2, data=data07d)
summary(model20)
summary(model21)
```

### Chapter08　最小二乗法による重回帰モデルの仮定と診断２

#### 8.1　仮定４：完全な多重共線性がないこと

##### 8.1.3　共変量における多重共線性


```{r 8.1.3.1, include=TRUE}
rm(list = ls())
data08a = read.csv("../../data/data08a.csv")
head(data08a)
summary(data08a)
```

$$ Y_{1} = 1 + 1.3X_{1} + 1.2X_{2} + 1.1X_{3} + e_{1} $$

```{r 8.1.3.2, include=TRUE}
model1a = lm(x1 ~ x2 + x3, data=data08a)
rj2a = summary(model1a)$r.squared
1 / (1 - rj2a)
model1b = lm(x2 ~ x1 + x3, data=data08a)
rj2b = summary(model1b)$r.squared
1 / (1 - rj2b)
model1c = lm(x3 ~ x1 + x2, data=data08a)
rj2c = summary(model1c)$r.squared
1 / (1 - rj2c)
```

```{r 8.1.3.3, include=TRUE}
model2 = lm(y1 ~ x1, data=data08a)
model3 = lm(y1 ~ x1 + x2, data = data08a)
model4 = lm(y1 ~ x1 + x3, data = data08a)
model5 = lm(y1 ~ x1 + x2 + x3, data = data08a)
summary(model2)
summary(model3)
summary(model4)
summary(model5)
```

```{r 8.1.3.4, include=TRUE}
library(car)
vif(model5)
```

#### 8.2　仮定５：誤差項の分散均一性

##### 8.2.2　不均一分散の診断

ブルーシュ・ペイガンの検定
<ol>
<li> 通常の最小二乗法による推定 </li>
<li> 残差を計算 </li>
<li> 残差の2乗を左辺に, 説明変数を右辺に置いた回帰モデルを通常の最小二乗法で推定 </li>
<li> 帰無仮説 </li>
<li> ステップ3で推定したモデルの決定係数$R_{bp}^{2}$を使って, LM統計量を$nR_{bp}^{2}$を計算 </li>
<li> $\chi^{2}$分布を用いてp値を計算 </li>
</ol>

```{r 8.2.2.1, include=TRUE}
rm(list = ls())
data08b = read.csv("../../data/data08b.csv")
head(data08b)
summary(data08b)
```

```{r 8.2.2.2, include=TRUE}
model1a = lm(y1 ~ x1, data = data08b)
model2a = lm(y2 ~ x1, data = data08b)
summary(model1a)
summary(model2a)
```

```{r 8.2.2.3, include=TRUE}
# ブルーシュ・ペイガンの検定
resid2a = residuals(model2a)  # step2
resid2b = resid2a^2   # step3
model2b = lm(resid2b ~ x1, data = data08b)   # step3
bp2 = summary(model2b)$r.squared * 1000   # step5
pchisq(bp2, 1, lower.tail = FALSE)
```

```{r 8.2.2.4, include=TRUE}
# ブルーシュ・ペイガンの検定（lmtest, bptestを使用）
library(lmtest)
bptest(model2a)
bptest(model2a)$p.value
```

##### 8.2.3　不均一分散への対処法１：加重最小二乗法

```{r 8.2.3.1, include=TRUE}
# 加重最小二乗法
model5 = lm(y2 ~ x1, weights = 1 / exp(1.5 * x1), data = data08b)
summary(model5)
```

実行可能一般化最小二乗法（FGLM: feasible generalized least squares）
<ol>
<li> 通常の最小二乗法による推定 </li>
<li> 残差を計算 </li>
<li> 残差の2乗を計算 </li>
<li> step3で計算した残差の2乗の自然対数を計算 </li>
<li> step4で計算した結果を左辺に置き, 説明変数を右辺に置いた回帰モデルを通常の最小二乗法で推定 </li>
<li> step5より, 予測値を計算 </li>
<li> step6の結果を指数変換したものを重みとして利用する </li>
</ol>

```{r 8.2.3.2, include=TRUE}
# 実行可能一般化最小二乗法
logresid2b = log(resid2b)
model6 = lm(logresid2b ~ x1, data = data08b)
hhat1 = predict(model6)
hhat2 = exp(hhat1)
model7 = lm(y2 ~ x1, weights = 1 / hhat2, data = data08b)
summary(model7)
```

##### 8.2.4　不均一分散への対処法２：不均一分散に頑健な標準誤差

```{r 8.2.4.1, include=TRUE}
model2a = lm(y2 ~ x1, data = data08b)
e1 = resid(model2a)
e2 = e1^2
hensax1 = data08b$x1 - mean(data08b$x1)
hensax2 = hensax1^2
num = sum(hensax2 * e2)
denom = (sum(hensax2))^2
sqrt(num / denom)
```

```{r 8.2.4.2, include=TRUE}
library(sandwich)
sqrt(vcovHC(model2a, type="HC"))
```

#### 8.3　仮定６：誤差項の正規性

### Chapter09　交互作用項のある共分散分析

#### 9.1　共分散分析の仮定

```{r 9.1.1, include=TRUE}
rm(list = ls())
data09 = read.csv("../../data/data09.csv")
head(data09)
summary(data09)
```

```{r 9.1.2, include=TRUE}
mean(data09$y1t) - mean(data09$y0t)
model1 = lm(y3 ~ x1 + t1, data = data09)
summary(model1)
```

#### 9.2　交互作用項のある共分散分析

```{r 9.2.1, include=TRUE}
x1t = data09$x1 * data09$t1
model2 = lm(y3 ~ x1 + t1 + x1t, data = data09)
summary(model2)
```


















